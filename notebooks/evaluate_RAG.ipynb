{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4a0616",
   "metadata": {},
   "source": [
    "Evaluation Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f921f656-1f08-4cef-aff8-416fc1ee21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_relevance_prompt=\"\"\"\n",
    "You are an expert evaluator for a Retrieval Augmented Generation system. Your task is to assess the relevance of the provided context for answering a given question.\n",
    "\n",
    "**Question**:\n",
    "{question}\n",
    "\n",
    "Retrieved Context:\n",
    "---\n",
    "{doc_1_content}\n",
    "---\n",
    "\n",
    "Instructions:\n",
    "Consider document in the retrieved context. Evaluate how relevant each document is to the user's **Question**.\n",
    "Then, provide an overall score for the entire set of retrieved context. The score should be a single floating-point number between 0.0 and 1.0, where 0.0 means the context is completely irrelevant or unhelpful, and 1.0 means the context is perfectly relevant and provides all necessary information to answer the question.\n",
    "\n",
    "Briefly explain your reasoning for the score, noting why document is most/least relevant.\n",
    "\n",
    "Please provide a response in a structured JSON format that matches the following format:\n",
    "{{\n",
    "  \"reasoning\": <Your brief explanation>,\n",
    "  \"score\": <Your Score as a float, e.g., 0.85>\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ebf2d64-2e79-4764-b01d-f87ffb9159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "faithfulness_prompt=\"\"\"\n",
    "You are an expert evaluator for a Retrieval Augmented Generation system. Your task is to assess the faithfulness of a generated answer to its provided context. An answer is faithful if all claims made in the answer are supported by the information present in the retrieved context. The answer should not make up information or contradict the context.\n",
    "\n",
    "Retrieved Context:\n",
    "---\n",
    "{doc_1_content}\n",
    "---\n",
    "\n",
    "Question (for reference):\n",
    "{question}\n",
    "\n",
    "Generated Answer:\n",
    "{generated_answer}\n",
    "\n",
    "Carefully compare the **Generated Answer** with the **Retrieved Context**. Determine if all factual statements in the answer can be verified from the context.\n",
    "Provide a score between 0.0 and 1.0, where:\n",
    "* 0.0 means the answer is completely unfaithful (e.g., contradicts the context or is entirely based on information outside the context).\n",
    "* 1.0 means the answer is perfectly faithful and all its claims are fully supported by the context.\n",
    "\n",
    "Identify any specific claims in the answer that are not supported by the context or contradict it. If the answer is fully faithful, state that.\n",
    "\n",
    "Please provide a response in a structured JSON format that matches the following format:\n",
    "{{\n",
    "  \"reasoning\": <Your brief explanation>,\n",
    "  \"score\": <Your Score as a float, e.g., 0.85>\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "706a9a7f-3667-44e2-8eb5-fdb1ac2042aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevance_to_question_prompt=\"\"\"\n",
    "You are an expert evaluator for a Retrieval Augmented Generation system. Your task is to assess how relevant and complete a generated answer is with respect to the user's question.\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "**Generated Answer:**\n",
    "{generated_answer}\n",
    "\n",
    "Evaluate the **Generated Answer** based on how well it addresses the **Question**. Consider the following:\n",
    "* **Directness:** Does the answer directly address the main intent of the question?\n",
    "* **Completeness:** Does the answer provide a reasonably complete response to the question, or does it miss key aspects?\n",
    "* **Focus:** Is the answer focused on the question, or does it include irrelevant information?\n",
    "\n",
    "Provide a score between 0.0 and 1.0, where:\n",
    "* 0.0 means the answer is completely irrelevant to the question or fails to address it at all.\n",
    "* 1.0 means the answer is perfectly relevant, directly addresses the question, and is comprehensive.\n",
    "\n",
    "Explain why the answer is or isn't relevant, noting any aspects of the question that were well-addressed or missed.\n",
    "\n",
    "Please provide a response in a structured JSON format that matches the following format:\n",
    "\n",
    "{{\n",
    "  \"reasoning\": <Your brief explanation>,\n",
    "  \"score\": <Your Score as a float, e.g., 0.85>\n",
    "}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2022c62c-4d37-49b2-994a-919b990bad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_correctness_score_prompt=\"\"\"\n",
    "You are an expert evaluator for a Retrieval Augmented Generation system. Your task is to assess the factual correctness and completeness of a **Generated Answer** by comparing it against a **Ground Truth Answer**.\n",
    "\n",
    "**Question:**\n",
    "{question}\n",
    "\n",
    "**Generated Answer:**\n",
    "{generated_answer}\n",
    "\n",
    "**Ground Truth Answer**\n",
    "{ground_truth_answer}\n",
    "\n",
    "**Instructions:**\n",
    "Carefully compare the **Generated Answer** with the **Ground Truth Answer**.\n",
    "Evaluate the **Generated Answer** based on the following criteria:\n",
    "* **Factual Accuracy:** Are all facts presented in the **Generated Answer** accurate when compared to the **Ground Truth Answer**? Does it introduce any inaccuracies or contradictions?\n",
    "* **Completeness:** Does the **Generated Answer** cover all the key information present in the **Ground Truth Answer** relevant to the question? Does it omit any critical details?\n",
    "* **Conciseness (Optional, if important):** Does the **Generated Answer** provide the information without unnecessary verbosity compared to the ground truth? (Consider if this is a primary evaluation goal).\n",
    "\n",
    "Provide an overall score for correctness between 0.0 and 1.0, where:\n",
    "* 0.0 means the **Generated Answer** is completely incorrect, factually inaccurate, or entirely misses the information present in the **Ground Truth Answer**.\n",
    "* 0.5 means the **Generated Answer** has some correct elements but also contains significant inaccuracies or omissions when compared to the **Ground Truth Answer**.\n",
    "* 1.0 means the **Generated Answer** is perfectly correct, factually accurate, and complete with respect to the **Ground Truth Answer**.\n",
    "\n",
    "**Reasoning (Optional but Recommended):**\n",
    "Explain your score by highlighting any factual inaccuracies, omissions, or (if evaluating conciseness) unnecessary information in the **Generated Answer** when compared to the **Ground Truth Answer**.\n",
    "\n",
    "Please provide a response in a structured JSON format that matches the following format:\n",
    "{{\n",
    "  \"reasoning\": <Your brief explanation>,\n",
    "  \"score\": <Your Score as a float, e.g., 0.5>\n",
    "}}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef85895a-90b3-46c9-af46-14e45de43723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ansvals=pd.read_parquet(\"datafiles/output_files/rag_generated_answers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7affb5d9-6255-4ae0-8700-8a8fef77d56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_category</th>\n",
       "      <th>retrieved_context</th>\n",
       "      <th>rag_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What percentage of 16 to 20-year-olds in the U...</td>\n",
       "      <td>81%</td>\n",
       "      <td>politics</td>\n",
       "      <td>UK youth 'interested' in politics\\n\\nThe major...</td>\n",
       "      <td>81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When did Portishead win the Mercury Music Prize?</td>\n",
       "      <td>1995</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>\"We've just had our heads down really, we've n...</td>\n",
       "      <td>The provided text does not state when Portishe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What animals are covered by Texas hunting laws...</td>\n",
       "      <td>State laws on hunting only covered \"regulated ...</td>\n",
       "      <td>tech</td>\n",
       "      <td>. \"Animals hit but not killed would without do...</td>\n",
       "      <td>Texas hunting laws cover regulated animals suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the slogan for the Conservative Party'...</td>\n",
       "      <td>\"It's not racist to impose limits on immigration\"</td>\n",
       "      <td>politics</td>\n",
       "      <td>The Tories have promised an upper limit on the...</td>\n",
       "      <td>\"It's not racist to impose limits on immigration\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who is the chairman of Wada?</td>\n",
       "      <td>Dick Pound</td>\n",
       "      <td>sport</td>\n",
       "      <td>Wada will appeal against ruling\\n\\nThe World A...</td>\n",
       "      <td>The provided text does not name the chairman o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What percentage of 16 to 20-year-olds in the U...   \n",
       "1   When did Portishead win the Mercury Music Prize?   \n",
       "2  What animals are covered by Texas hunting laws...   \n",
       "3  What is the slogan for the Conservative Party'...   \n",
       "4                       Who is the chairman of Wada?   \n",
       "\n",
       "                                              answer source_category  \\\n",
       "0                                                81%        politics   \n",
       "1                                               1995   entertainment   \n",
       "2  State laws on hunting only covered \"regulated ...            tech   \n",
       "3  \"It's not racist to impose limits on immigration\"        politics   \n",
       "4                                         Dick Pound           sport   \n",
       "\n",
       "                                   retrieved_context  \\\n",
       "0  UK youth 'interested' in politics\\n\\nThe major...   \n",
       "1  \"We've just had our heads down really, we've n...   \n",
       "2  . \"Animals hit but not killed would without do...   \n",
       "3  The Tories have promised an upper limit on the...   \n",
       "4  Wada will appeal against ruling\\n\\nThe World A...   \n",
       "\n",
       "                                          rag_answer  \n",
       "0                                                81%  \n",
       "1  The provided text does not state when Portishe...  \n",
       "2  Texas hunting laws cover regulated animals suc...  \n",
       "3  \"It's not racist to impose limits on immigration\"  \n",
       "4  The provided text does not name the chairman o...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansvals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31913537-a048-40b5-b611-231d18ace6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24c48015-03c9-4048-8b91-71cfd5353d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_model_name = \"gemini_text-embedding-004\"\n",
    "generator_model_name = \"gemini-1.5-flash\"\n",
    "retriever_top_k_config = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6b7c9-e87a-4b32-8650-fe75758f6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset=ansvals[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc075a94",
   "metadata": {},
   "source": [
    "Defining Metrics call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca6e98a9-f94c-49b2-ad77-268bc2b23e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"<gemini api key>\"\n",
    "judge_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "class JudgeResponse(BaseModel):\n",
    "    reasoning: str\n",
    "    score: str\n",
    "\n",
    "structured_llm = judge_llm.with_structured_output(JudgeResponse)\n",
    "\n",
    "def call_llm(prompt):\n",
    "    response = structured_llm.invoke(prompt)\n",
    "    return response\n",
    "    \n",
    "def calculate_context_relevance(question,context):\n",
    "    promptval=context_relevance_prompt.format(question=question,doc_1_content=context)\n",
    "    response=call_llm(promptval)\n",
    "    return response\n",
    "\n",
    "def calculate_faithfulness(question,context,generated_answer):\n",
    "    promptval=faithfulness_prompt.format(question=question,doc_1_content=context,generated_answer=generated_answer)\n",
    "    response=call_llm(promptval)\n",
    "    return response\n",
    "\n",
    "def calculate_answer_relevance_to_question(question,generated_answer):\n",
    "    promptval=answer_relevance_to_question_prompt.format(question=question,generated_answer=generated_answer)\n",
    "    response=call_llm(promptval)\n",
    "    return response\n",
    "\n",
    "def calculate_answer_correctness_score(question,generated_answer,ground_truth_answer):\n",
    "    promptval=answer_correctness_score_prompt.format(question=question,generated_answer=generated_answer,ground_truth_answer=ground_truth_answer)\n",
    "    response=call_llm(promptval)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b87d89b0-d8e8-4cca-a451-baedab363607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question             What percentage of 16 to 20-year-olds in the U...\n",
       "answer                                                             81%\n",
       "source_category                                               politics\n",
       "retrieved_context    UK youth 'interested' in politics\\n\\nThe major...\n",
       "rag_answer                                                         81%\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ansvals.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "393d3de4-1c26-4654-bfb6-d51bcd490fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JudgeResponse(reasoning='The retrieved context directly answers the question. It states that 81% of 16 to 20-year-olds in the UK feel strongly about issues like crime and education. Therefore, the context is highly relevant.', score='1.0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_context_relevance(ansvals.iloc[0][\"question\"],ansvals.iloc[0][\"retrieved_context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dac4bca1-51ce-432a-8bbf-2b21028a061e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JudgeResponse(reasoning='The answer states that 81% of 16 to 20-year-olds in the UK feel strongly about issues like crime and education. The context states that research suggests 81% of 16 to 20-year-olds feel strongly about issues like crime and education. Therefore, the answer is faithful to the context.', score='1.0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_faithfulness(ansvals.iloc[0][\"question\"],ansvals.iloc[0][\"retrieved_context\"],ansvals.iloc[0][\"rag_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "986d13bb-614d-4ed8-851d-cf0b4057fd90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JudgeResponse(reasoning=\"The answer provides a percentage that directly answers the question about the proportion of 16 to 20-year-olds in the UK who feel strongly about issues like crime and education. It's concise and focused.\", score='1.0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_answer_relevance_to_question(ansvals.iloc[0][\"question\"],ansvals.iloc[0][\"rag_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92a54542-466b-4b49-8038-afba8413581a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JudgeResponse(reasoning='The generated answer is factually accurate and complete when compared to the ground truth answer. It provides the correct percentage.', score='1.0')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_answer_correctness_score(ansvals.iloc[0][\"question\"],ansvals.iloc[0][\"rag_answer\"],ansvals.iloc[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "59618ed0-f057-42f9-af07-522971b8bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset=ansvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae36410",
   "metadata": {},
   "source": [
    "MLFlow Evaluation Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c8aaf94a-ef0a-4891-969f-45f6170cab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace(name=\"RAG.evaluate_question_pipeline\")\n",
    "def get_metrics(question,retrieved_context,generated_answer,ground_truth_answer):\n",
    "    context_relevance = calculate_context_relevance(question, retrieved_context)\n",
    "    faithfulness = calculate_faithfulness(question, retrieved_context, generated_answer)\n",
    "    answer_relevance_q = calculate_answer_relevance_to_question(question,generated_answer)\n",
    "    correctness = calculate_answer_correctness_score(question, generated_answer, ground_truth_answer)\n",
    "    return {\"context_relevance\":context_relevance,\n",
    "            \"faithfulness\":faithfulness,\n",
    "            \"answer_relevance_q\":answer_relevance_q,\n",
    "            \"correctness\":correctness}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b7479bd1-58b7-4c44-9d23-2f493da07fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Run ID: 63d4820eee494ad1a73f2910ac3401be\n",
      "MLflow Experiment ID: 0\n",
      "🏃 View run RAG_Eval_20250606_234705 at: http://localhost:5000/#/experiments/0/runs/63d4820eee494ad1a73f2910ac3401be\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=2c0078fb90e0452492b9b6c66d50a40e&amp;experiment_id=0&amp;trace_id=9600426b54414c75a6b3f646f617c325&amp;experiment_id=0&amp;trace_id=af439a629b8641b392dbcd5c993f6919&amp;experiment_id=0&amp;trace_id=3f0373df19ec412ca67a9ff44f31cb1b&amp;experiment_id=0&amp;trace_id=0763d87611e945468687c3ecda083d5f&amp;experiment_id=0&amp;trace_id=fb69bcb9f6d8419a8701a93ef6cfaa6b&amp;experiment_id=0&amp;trace_id=a7a2a6a294ed4e3297d53414f65e004a&amp;experiment_id=0&amp;trace_id=54cced42083f403680d240c6a3266854&amp;experiment_id=0&amp;trace_id=653d01a4f02b4fe4af9782fa64a10c6b&amp;experiment_id=0&amp;trace_id=3bacb523bdb747d58b0824bcd34d1f9a&amp;experiment_id=0&amp;version=2.22.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=2c0078fb90e0452492b9b6c66d50a40e), Trace(request_id=9600426b54414c75a6b3f646f617c325), Trace(request_id=af439a629b8641b392dbcd5c993f6919), Trace(request_id=3f0373df19ec412ca67a9ff44f31cb1b), Trace(request_id=0763d87611e945468687c3ecda083d5f), Trace(request_id=fb69bcb9f6d8419a8701a93ef6cfaa6b), Trace(request_id=a7a2a6a294ed4e3297d53414f65e004a), Trace(request_id=54cced42083f403680d240c6a3266854), Trace(request_id=653d01a4f02b4fe4af9782fa64a10c6b), Trace(request_id=3bacb523bdb747d58b0824bcd34d1f9a)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "with mlflow.start_run(run_name=f\"RAG_Eval_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "        print(f\"MLflow Run ID: {run.info.run_id}\")\n",
    "        print(f\"MLflow Experiment ID: {run.info.experiment_id}\")\n",
    "\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"retriever_model\", retriever_model_name)\n",
    "        mlflow.log_param(\"generator_model\", generator_model_name)\n",
    "        mlflow.log_param(\"retriever_top_k\", retriever_top_k_config)\n",
    "        mlflow.log_param(\"evaluation_dataset_size\", len(evaluation_dataset))\n",
    "        context_relevance_score=0\n",
    "        faithfulness_score=0\n",
    "        answer_relevance_q_score=0\n",
    "        correctness_score=0\n",
    "        for i, item in evaluation_dataset.iterrows():\n",
    "            question_id = i+1\n",
    "            question = item[\"question\"]\n",
    "            ground_truth_answer = item[\"answer\"]\n",
    "            retrieved_context = item[\"retrieved_context\"] \n",
    "            generated_answer = item[\"rag_answer\"]\n",
    "            \n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics_response=get_metrics(question,retrieved_context,generated_answer,ground_truth_answer)\n",
    "            context_relevance = metrics_response[\"context_relevance\"]\n",
    "            faithfulness= metrics_response[\"faithfulness\"]\n",
    "            answer_relevance_q= metrics_response[\"answer_relevance_q\"]\n",
    "            correctness= metrics_response[\"correctness\"]\n",
    "\n",
    "            context_relevance_score+=float(context_relevance.score)\n",
    "            faithfulness_score+=float(faithfulness.score)\n",
    "            answer_relevance_q_score+=float(answer_relevance_q.score)\n",
    "            correctness_score+=float(correctness.score)\n",
    "        total_items=len(evaluation_dataset)\n",
    "        mlflow.log_metric(f\"avg_context_relevance\",context_relevance_score/total_items)\n",
    "        mlflow.log_metric(f\"avg_faithfulness\", faithfulness_score/total_items)\n",
    "        mlflow.log_metric(f\"avg_answer_relevance_q\", answer_relevance_q_score/total_items)\n",
    "        mlflow.log_metric(f\"avg_similarity_gt\", correctness_score/total_items)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a15506b4-bb9d-4856-8f84-0c2109a4911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evaluation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bfbb7d-591b-43d1-9f5a-05b514715f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = len(evaluation_dataset)\n",
    "avg_metrics = {k: v / num_items if num_items > 0 else 0 for k, v in metric_totals.items()}\n",
    "for metric_name, avg_value in avg_metrics.items():\n",
    "    mlflow.log_metric(f\"avg_{metric_name}_llm\", avg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094c545-73ef-4a46-8240-3de27f7192ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
